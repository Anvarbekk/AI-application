{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyP4wi0ewghNEitJViG9ofGO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anvarbekk/AI-application/blob/main/week_6_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lR5IT3iFAXdN"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow==1.15.5\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "GpOSDzB0Aulg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_parameter = 0.01\n",
        "epochs = 300"
      ],
      "metadata": {
        "id": "u00qZvzhAz2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_points = 50\n",
        "x_train = np.linspace(0,30,sample_points)\n",
        "y_train = 6*x_train + 7*np.random.randn(sample_points)"
      ],
      "metadata": {
        "id": "NrxRlOhrA3xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Noisy dataset\n",
        "plt.plot(x_train, y_train, 'o')\n",
        "# Noise free dataset \n",
        "plt.plot(x_train, 6*x_train)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W460skKuA8Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = tf.placeholder(tf.float32)\n",
        "X = tf.placeholder(tf.float32)\n",
        "\n",
        "W = tf.Variable(np.random.randn(), name = 'weights')\n",
        "B = tf.Variable(np.random.randn(), name = 'bias')"
      ],
      "metadata": {
        "id": "yfvwulmaBAKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the model for regression\n",
        "prediction = W*X + B\n",
        "\n",
        "# Cost function\n",
        "cost_iteration = tf.reduce_sum((prediction-Y)**2)/(2*sample_points)\n",
        "\n",
        "#Define the optimizer\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_parameter).minimize(cost_iteration)\n",
        "\n",
        "# Initialize the variables\n",
        "init = tf.global_variables_initializer()"
      ],
      "metadata": {
        "id": "CW_UkPqVBGRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for epoch in range(epochs):\n",
        "    for x, y in zip(x_train, y_train):\n",
        "      sess.run(optimizer, feed_dict = {X : x, Y : y})\n",
        "    if not epoch%40:\n",
        "      W1 = sess.run(W)\n",
        "      B1 = sess.run(B)\n",
        "      cost_iter = sess.run(cost_iteration, feed_dict = {X : x, Y : y})\n",
        "      print('Epochs %f Cost %f Weight %f Bias %f' %(epoch, cost_iter, W1, B1))\n",
        "  Weight = sess.run(W)\n",
        "  Bias = sess.run(B)\n",
        "\n",
        "  plt.plot(x_train, y_train, 'o')\n",
        "  plt.plot(x_train,Weight*x_train+Bias)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "yOKVRBcKgLsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model for regression\n",
        "with tf.name_scope(\"Model\") as scope:\n",
        "  prediction = W*X + B\n",
        "\n",
        "# Add summary to study behaviour of weights and biases with epochs\n",
        "weight_histogram = tf.summary.histogram(\"Weights\", W)\n",
        "bias_histogram = tf.summary.histogram(\"Bias\", B)\n",
        "\n",
        "# Cost function\n",
        "with tf.name_scope(\"Cost_function\") as scope:\n",
        "  cost_iteration = tf.reduce_sum((prediction-Y)**2)/(2*sample_points)\n",
        "\n",
        "# Record the scalar summary of the cost function\n",
        "cost_summary = tf.summary.scalar(\"Cost\", cost_iteration)"
      ],
      "metadata": {
        "id": "cmO3SAU6iN17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the optimizer\n",
        "with tf.name_scope(\"Training\") as scope:\n",
        "  optimizer = tf.train.GradientDescentOptimizer(learning_parameter).minimize(cost_iteration)\n",
        "\n",
        "# Initialize the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "#Merge all the summaries into a single operator\n",
        "merged_summary = tf.summary.merge_all()"
      ],
      "metadata": {
        "id": "_U6G3sXPiSot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tensorflow session\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  writer = tf.summary.FileWriter('./log', sess.graph)\n",
        "  for epoch in range(epochs):\n",
        "    for x, y in zip(x_train, y_train):\n",
        "      sess.run(optimizer, feed_dict = {X : x, Y : y})\n",
        "\n",
        "      # Write logs for each epochs\n",
        "      summary_epochs = sess.run(merged_summary, feed_dict = {X : x, Y : y})\n",
        "      writer.add_summary(summary_epochs, epoch)\n",
        "    if not epoch%40:\n",
        "      W1 = sess.run(W)\n",
        "      B1 = sess.run(B)\n",
        "      cost_iter = sess.run(cost_iteration, feed_dict = {X : x, Y : y})\n",
        "      print('Epochs %f Cost %f Weight %f Bias %f' %(epoch, cost_iter, W1, B1))\n",
        "  Weight = sess.run(W)\n",
        "  Bias = sess.run(B)\n",
        "\n",
        "  plt.plot(x_train, y_train, 'o')\n",
        "  plt.plot(x_train,Weight*x_train+Bias)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "3u4AAt9xiXaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "UE20yKXipmBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist_data = input_data.read_data_sets(\"./data\", one_hot = True)"
      ],
      "metadata": {
        "id": "wjM-GWx3pvVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_training = mnist_data.train.num_examples\n",
        "num_testing = mnist_data.test.num_examples\n",
        "num_validation = mnist_data.validation.num_examples\n",
        "print(\"MNIST Datasize: Training samples: {0}, Testing samples: {1}\")"
      ],
      "metadata": {
        "id": "zYASdqqup2l3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network parameters of Neural Network\n",
        "n_input = 784     # Input image of size 28 x 28\n",
        "n_hidden_1 = 512  # First hidden layer\n",
        "n_hidden_2 = 256  # Second hidden layer\n",
        "n_hidden_3 = 128  # Third hidden layer\n",
        "n_output = 10     # Output layer having (0-9) digits"
      ],
      "metadata": {
        "id": "-wF_OpGQp6sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "epochs = 3000\n",
        "batch_size = 128\n",
        "keep_prob = tf.placeholder(tf.float32)"
      ],
      "metadata": {
        "id": "uBR245BJqCaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building tensorflow graph\n",
        "X = tf.placeholder(tf.float32, [None, n_input])\n",
        "Y = tf.placeholder(tf.float32, [None, n_output])"
      ],
      "metadata": {
        "id": "7nP00Xk-qDJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_weight = {\"W1\": tf.Variable(tf.truncated_normal([n_input, n_hidden_1], stddev = 0.1)),\n",
        "             \"W2\": tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2], stddev = 0.1)),\n",
        "             \"W3\": tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3], stddev = 0.1)),\n",
        "             \"Wout\":tf.Variable(tf.truncated_normal([n_hidden_3, n_output]))\n",
        "}\n",
        "\n",
        "nn_bias = { \"B1\": tf.Variable(tf.truncated_normal([n_hidden_1])),\n",
        "            \"B2\": tf.Variable(tf.truncated_normal([n_hidden_2])),\n",
        "            \"B3\": tf.Variable(tf.truncated_normal([n_hidden_3])),\n",
        "            \"B4\": tf.Variable(tf.truncated_normal([n_output])),  \n",
        "           }"
      ],
      "metadata": {
        "id": "SZvVLwnUqDr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_layer_1 = tf.add(tf.matmul(X, nn_weight[\"W1\"]),nn_bias[\"B1\"])\n",
        "nn_layer_2 = tf.add(tf.matmul(nn_layer_1, nn_weight[\"W2\"]),nn_bias[\"B2\"])\n",
        "nn_layer_3 = tf.add(tf.matmul(nn_layer_2, nn_weight[\"W3\"]),nn_bias[\"B3\"])\n",
        "layer_drop = tf.nn.dropout(nn_layer_3, keep_prob)\n",
        "output_layer = tf.add(tf.matmul(layer_drop, nn_weight[\"Wout\"]), nn_bias[\"B4\"])"
      ],
      "metadata": {
        "id": "ch_9EzJlqIrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss\n",
        "computed_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = output_layer, labels = Y))\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(computed_loss)\n",
        "\n",
        "#Define prediction\n",
        "prediction_out = tf.equal(tf.argmax(output_layer,1), tf.argmax(Y,1))\n",
        "\n",
        "# Define accuracy of the model\n",
        "nn_accuracy = tf.reduce_mean(tf.cast(prediction_out, tf.float32))\n",
        "\n",
        "# Initialize all the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "saver = tf.train.Saver()"
      ],
      "metadata": {
        "id": "MVl2dQENqdui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for i in range(epochs):\n",
        "\n",
        "    mini_batch_x, mini_batch_y = mnist_data.train.next_batch(batch_size)\n",
        "    #print(mini_batch_x[0:1,:].shape)\n",
        "    mini_batch_val_x, mini_batch_val_y = mnist_data.validation.next_batch(batch_size)\n",
        "\n",
        "    sess.run(optimizer, feed_dict = {X : mini_batch_x, Y : mini_batch_y, keep_prob:1})\n",
        "    \n",
        "    if i%100 == 0:\n",
        "      mini_batch_loss, mini_batch_accuracy = sess.run([computed_loss, nn_accuracy], feed_dict = {X: mini_batch_x, Y: mini_batch_y, keep_prob:1})\n",
        "\n",
        "      mini_batch_val_loss, mini_batch_val_accuracy = sess.run([computed_loss, nn_accuracy], feed_dict = {X: mini_batch_x, Y: mini_batch_y, keep_prob:1})\n",
        "\n",
        "      print(\"Iterations : {0} , Train_loss = {1}, Train_Accuracy {2}, Val_loss {3}, Val_accuracy {4}\".format(i, mini_batch_loss, mini_batch_accuracy, mini_batch_val_loss, mini_batch_val_accuracy))\n",
        "\n",
        "  print(\"Optimization Finished\")\n",
        "  test_accuracy = sess.run(nn_accuracy, feed_dict = {X:mnist_data.test.images, Y:mnist_data.test.labels, keep_prob:1.0})\n",
        "  print(\"Testing accuracy is {0}\".format(test_accuracy))\n",
        "\n",
        "  saver_path = saver.save(sess, \"./model/my_model.ckpt\")"
      ],
      "metadata": {
        "id": "eE6RsZcMqhrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread(\"7.jpg\")\n",
        "gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "rescaled_image = cv2.resize(gray_image, (28,28))\n",
        "plt.imshow(rescaled_image, cmap = 'gray')\n",
        "plt.show()\n",
        "rescaled_image.shape\n",
        "#test_image = rescaled_image.flatten()\n",
        "\n",
        "dum = rescaled_image.reshape(1,-1)/255\n",
        "dum.shape\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess, \"./model/my_model.ckpt\")\n",
        "  Z = output_layer.eval(feed_dict = {X:dum, keep_prob:1.0})\n",
        "  y_pred = np.argmax(Z, axis = 1)\n",
        "  print(\"Prediction for test image is {0}\".format(y_pred))"
      ],
      "metadata": {
        "id": "jwNDGulnrNlk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}